{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f400785e-444d-4ede-ac51-249aaaa727ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58547775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 102\n",
      "Number of classes: 102\n",
      "Number of classes: 819\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"dataset\"\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "valid_dir = os.path.join(base_dir, \"valid\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "print(\"Number of classes:\", len(os.listdir(train_dir)))\n",
    "print(\"Number of classes:\", len(os.listdir(valid_dir)))\n",
    "print(\"Number of classes:\", len(os.listdir(test_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1808b789",
   "metadata": {},
   "source": [
    "## Step 1: Data Preproccessing\n",
    "The first step is data preprocessing, where clean and prepare the dataset ready to apply for machine learning model. This step is important to ensure data quality and reliable model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64506696",
   "metadata": {},
   "source": [
    "### Image Size Inspection\n",
    "\n",
    "Before preprocessing, several sample images were inspected to examine their original dimensions. The images were found to have different size which makes them unsuitable for direct input into machine learning models. Therefore, resizing is required to standardize the input dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23df4e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image sizes (width, height): [(523, 500), (666, 500), (750, 500), (595, 500), (626, 500)]\n"
     ]
    }
   ],
   "source": [
    "limits = 5\n",
    "\n",
    "sizes = []\n",
    "for cls in os.listdir(train_dir):\n",
    "    image_files = os.listdir(os.path.join(train_dir, cls))[:limits]\n",
    "\n",
    "    for image_name in image_files:\n",
    "        image_path = os.path.join(train_dir, cls, image_name)\n",
    "        image = Image.open(image_path)\n",
    "        sizes.append(image.size)\n",
    "\n",
    "        if(len(sizes)) >= limits:\n",
    "            break\n",
    "    if(len(sizes)) >= limits:\n",
    "        break\n",
    "\n",
    "print(\"Image sizes (width, height):\", sizes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd39443",
   "metadata": {},
   "source": [
    "### Image Cleaning and Resizing\n",
    "\n",
    "Next, the preprocess_image function will:\n",
    "1. Resized all images to 64*64 pixels to ensure a fixed length input for machine learning model. \n",
    "2. Converted images to RGB format.\n",
    "3. Normalize pixel values to improve training stability.\n",
    "4. Flattens each images into 1-dimensional array for input to machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c0705fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed data shape: (6552, 12288)\n",
      "Labels shape: (6552,)\n"
     ]
    }
   ],
   "source": [
    "def preprocess_image(folder, IMAGE_SIZE = (64, 64)):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    class_folder = [f \n",
    "                    for f in os.listdir(folder) \n",
    "                        if os.path.isdir(os.path.join(folder, f)\n",
    "                    )]\n",
    "\n",
    "    if class_folder:\n",
    "        for label in class_folder:\n",
    "            label_path = os.path.join(folder, label)\n",
    "\n",
    "            for image_name in os.listdir(label_path):\n",
    "                image_path = os.path.join(label_path, image_name)\n",
    "                try:\n",
    "                    image = Image.open(image_path).convert('RGB')\n",
    "                    image = image.resize(IMAGE_SIZE)\n",
    "                    image = np.array(image) / 255.0\n",
    "                    image = image.flatten()\n",
    "                    x.append(image)\n",
    "                    y.append(label)\n",
    "                except:\n",
    "                    continue\n",
    "    else:\n",
    "        for image_name in os.listdir(folder):\n",
    "            image_path = os.path.join(folder, image_name)\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image = image.resize(IMAGE_SIZE)\n",
    "                image = np.array(image) / 255.0\n",
    "                image = image.flatten()\n",
    "                x.append(image)\n",
    "                y.append(\"unknown\")\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    return np.array(x), np.array(y)\n",
    "\n",
    "x, y = preprocess_image(train_dir)\n",
    "print(\"Preprocessed data shape:\", x.shape)\n",
    "print(\"Labels shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e98a59d",
   "metadata": {},
   "source": [
    "### Preprocessing Train, Validation, and Test Sets\n",
    "The training, validation, and test datasets were preprocessed using the preprocess_image function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de64eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (6552, 12288) (6552,)\n",
      "Validation data shape: (818, 12288) (818,)\n",
      "Test data shape: (819, 12288) (819,)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train = preprocess_image(train_dir)\n",
    "X_valid, Y_valid = preprocess_image(valid_dir)\n",
    "X_test, Y_test = preprocess_image(test_dir)\n",
    "\n",
    "print(\"Training data shape:\", X_train.shape, Y_train.shape)\n",
    "print(\"Validation data shape:\", X_valid.shape, Y_valid.shape)\n",
    "print(\"Test data shape:\", X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7ede52",
   "metadata": {},
   "source": [
    "### Encode Labels\n",
    "Class label were converted from string format to numeric labels for model compatibality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fa664",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "Y_train_enc = le.fit_transform(Y_train)\n",
    "Y_valid_enc = le.transform(Y_valid)\n",
    "Y_test_enc = le.transform(Y_test)\n",
    "\n",
    "print(\"Classes (First 10): \", le.classes_[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be10326",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "All input features were standardized to have zero mean and unit variance to improve neural network performance during training and ensures that all input features contributes equally to the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b8db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a8568bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 6552\n",
      "Valid images: 818\n",
      "Test images: 0\n"
     ]
    }
   ],
   "source": [
    "def count_images(folder):\n",
    "    total = 0\n",
    "    for f in os.listdir(folder):\n",
    "        path = os.path.join(folder, f)\n",
    "        if os.path.isdir(path):\n",
    "            total += len(os.listdir(path))\n",
    "    return total\n",
    "\n",
    "print(\"Train images:\", count_images(train_dir))\n",
    "print(\"Valid images:\", count_images(valid_dir))\n",
    "print(\"Test images:\", count_images(test_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c920e",
   "metadata": {},
   "source": [
    "## Step 2: Model Selection\n",
    "We select three complementary models to justify different biases/complexities:\n",
    "- Logistic Regression: strong linear baseline for multiclass classification.\n",
    "- Support Vector Classifier (RBF kernel): non-linear decision boundaries without deep nets.\n",
    "- MLP (ANN): a shallow neural network over flattened pixels.\n",
    "Each model will use `StandardScaler` to normalize features for more stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9cf33c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models ready: ['LogisticRegression', 'SVC_RBF', 'MLP_ANN']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define three models (scaling is done in Step 1 preprocessing)\n",
    "models = {\n",
    "    \"LogisticRegression\": Pipeline([\n",
    "        (\"clf\", LogisticRegression(max_iter=1000, solver=\"lbfgs\"))\n",
    "    ]),\n",
    "    \"SVC_RBF\": Pipeline([\n",
    "        (\"clf\", SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\"))\n",
    "    ]),\n",
    "    \"MLP_ANN\": Pipeline([\n",
    "        (\"clf\", MLPClassifier(hidden_layer_sizes=(256, 128), activation=\"relu\", max_iter=50, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "print(\"Models ready:\", list(models.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CDS6314",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
